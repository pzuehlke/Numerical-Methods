{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdc50eb",
   "metadata": {},
   "source": [
    "$ \\newcommand{\\mbf}{\\mathbf} $\n",
    "$ \\newcommand{\\abs}[1]{\\left\\vert#1\\right\\vert} $\n",
    "$ \\newcommand{\\se}[1]{\\,\\left\\{#1\\right\\}\\,} $\n",
    "$ \\newcommand{\\norm}[1]{\\left\\Vert#1\\right\\Vert} $\n",
    "# Mínimos quadrados com ajuste linear: caso geral\n",
    "\n",
    "Sejam $ f_0,\\,f_1, \\dots,\\,f_n $ funções quaisquer de $ x $ e \n",
    "$$\n",
    "(x_0, y_0),\\, (x_1, y_1), \\cdots , (x_M,y_M)\n",
    "$$\n",
    "um conjunto de dados, com $ n < M $. Finalmente, seja \n",
    "\\begin{equation*}\\label{E:comb}\n",
    "    f(x) = a_0f_0(x) + a_1f_1(x) + \\cdots + a_nf_n(x) \\tag{1}\n",
    "\\end{equation*}\n",
    "uma combinação linear das **funções-base** $ f_k $.\n",
    "\n",
    "Gostaríamos de encontrar a função $ f $ da forma \\eqref{E:comb} que \"melhor se adapta\" aos dados. Seguindo o método dos mínimos quadrados, precisamos escolher os coeficientes $ a_k $ de maneira a minimizar a soma dos quadrados dos **resíduos** $ r_i = y_i - f(x_i) $:\n",
    "$$\n",
    "S(a_0,a_1,\\dots,a_n) = \\sum_{i=0}^M \\big[y_i - f(x_i)\\big]^2\\,.\n",
    "$$\n",
    "Num ponto que minimiza $ S $, todas as derivadas parciais devem se anular, ou seja:\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial a_k} = 2 \\sum_{i=0}^M \\big[f(x_i) - y_i\\big]f_k(x_i) = 0 \\qquad (\\text{para cada }k = 0, 1, \\dots, n)\\,.\n",
    "$$\n",
    "Equivalentemente:\n",
    "\\begin{alignat*}{9}\n",
    "\\sum_{i=0}^M f(x_i)f_k(x_i) &= \\sum_{i=0}^M \\Bigg(\\sum_{j=0}^n a_j f_j(x_i)\\Bigg) f_k(x_i) \\\\\n",
    "&= \\sum_{j=0}^n \\Bigg(\\sum_{i=0}^M f_k(x_i)f_j(x_i)\\Bigg)a_j \\\\\n",
    "&= \\sum_{i=0}^My_if_k(x_i) \\qquad \\qquad \\qquad (\\text{para cada }k = 0, 1, \\dots, n)\\,.\n",
    "\\end{alignat*}\n",
    "Na forma matricial, o último conjunto de equações pode ser reescrito como\n",
    "\\begin{equation*}\\label{E:normal}\n",
    "\\boxed{\\mbf C\\mbf a = \\mbf b \\quad \\text{onde}\\quad \\mbf C = \\big(c_{kj}\\big) = \\bigg( \\sum_{i=0}^M f_k(x_i)f_j(x_i) \\bigg)\\,, \\quad \\mbf a = \\big(a_j\\big) \\quad \\text{e} \\quad \\mbf b = \\bigg (\\sum_{i=0}^M f_k(x_i) \\bigg)} \\tag{2}\n",
    "\\end{equation*}\n",
    "para $ k,\\,j = 0, 1, \\dots, n $.\n",
    "\n",
    "Assim como no caso especial da regressão polinomial (em que $ f_k(x) = x^k $), as equações \\eqref{E:normal} são conhecidas como **equações normais**. Observe que $ \\mbf C $ é uma matriz simétrica $ (n + 1) \\times (n + 1) $ e que o sistema acima é *linear* nas variáveis $ a_0,\\, a_1, \\cdots,\\,a_n $. Discutiremos na próxima seção sob quais condições este sistema tem uma solução única."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7a5ba",
   "metadata": {},
   "source": [
    "## $ \\S 5 $ Interpretação geométrica alternativa para o caso linear dos mínimos quadrados\n",
    "\n",
    "O objetivo desta seção é discutir uma interpretação alternativa para os coeficientes $ \\hat{\\mbf a} = \\big(\\hat a_0,\\,\\hat{a}_1, \\dots, \\hat{a}_n\\big) $ que minimizam a soma $ S $ dos quadrados dos resíduos.\n",
    "\n",
    "Sejam \n",
    "$$\n",
    "\\mbf y = (y_0,y_1,\\dots,y_{M}) \\quad \\text{e} \\quad \\mbf v_k = \\big(f_k(x_1),\\,f_k(x_2), \\dots, f_k(x_M)\\big) \\in \\mathbb R^{M+1} \\qquad (k = 0,\\,1, \\dots, n)\\,.\n",
    "$$\n",
    "Em termos destes vetores, a função $ S $ que gostaríamos de minimizar pode ser escrita como\n",
    "$$\n",
    "S(a, b) = \\sum_{i=0}^M \\big[y_i - f(x_i)\\big]^2 = \\norm{\\mbf y - \\bigg(\\sum_{k=0}^n a_k\\mbf v_k \\bigg)}^2\\,,\n",
    "$$\n",
    "onde $ \\norm{\\cdot} $ denota a norma euclidiana em $ \\mathbb R^{M+1} $. Minimizar $ S $  é equivalente a escolher os coeficientes $ a_k $ na combinação linear\n",
    "$$\n",
    "\\sum_{k=0}^n a_k \\mbf v_k\n",
    "$$\n",
    "de modo que a distância do vetor resultante a $ \\mbf y $ seja a menor possível.\n",
    "\n",
    "Como por hipótese os $ x_i $ não são todos iguais, $ \\mbf x $ e $ \\mbf u $ são linearmente independentes, logo geram um plano $ P $ de dimensão $ 2 $ dentro de $ \\mathbb R^{M+1} $. Podemos decompor $ \\mbf y $ como\n",
    "$$\n",
    "\\mbf y = \\mbf y^\\perp + (\\mbf y - \\mbf y^\\perp)\\,,\n",
    "$$\n",
    "onde $ \\mbf y^\\perp $ é a componente de $ \\mbf y $ ortogonal a $ P $ e $ \\mbf {\\bar y} = (\\mbf y - \\mbf y^\\perp) $ a componente que pertence a ele, ou equivalentemente a projeção ortogonal de $ \\mbf y $ em $ P $. Então\n",
    "\\begin{alignat*}{9}\n",
    "S(a,b) &= \\norm{\\mbf y - (a\\,\\mbf x + b \\,\\mbf u)}^2 \\\\\n",
    "&= \\norm{\\mbf y^\\perp + (\\mbf y - \\mbf y^\\perp) - (a\\,\\mbf x + b \\,\\mbf u)}^2 \\\\\n",
    "&= \\norm{\\mbf y^\\perp}^2 + \\norm{\\mbf {\\bar y} - (a\\,\\mbf x + b \\,\\mbf u)}^2\n",
    "\\end{alignat*}\n",
    "pelo teorema de Pitágoras. Agora, como $ \\mbf {\\bar y} \\in P $ e $ \\mbf x $ e $ \\mbf u $ são l.i., existem *únicos* $ \\hat a $ e $ \\hat b $ em $ \\mathbb R $ tais que\n",
    "$$\n",
    "\\mbf {\\bar y} = \\hat a\\,\\mbf x + \\hat b\\,\\mbf u\\,.\n",
    "$$\n",
    "Este par $ (\\hat a, \\hat b) $ é portanto o único ponto de mínimo global de $ S $. Além disto, a menor soma dos quadrados dos resíduos é dada por\n",
    "$$\n",
    "S(\\hat a, \\hat b) = \\norm{\\mbf y^\\perp}^2\\,.\n",
    "$$\n",
    "\n",
    "Este argumento fornece não só uma demonstração de que a reta de mínimos quadrados existe, mas uma maneira de encontrá-la: basta projetar $ \\mbf y $ ortogonalmente no plano $ P $ e resolver a equação linear\n",
    "$$\n",
    "\\mbf {\\bar y} = a\\,\\mbf x + b\\,\\mbf u\\,.\n",
    "$$\n",
    "O desvio padrão é dado por\n",
    "$$\n",
    "\\sigma = \\frac{\\norm{\\mbf y^\\perp}}{\\sqrt{M - 1}}\\,.\n",
    "$$\n",
    "\n",
    "### $ 5.2 $ Fórmula para a projeção ortogonal\n",
    "\n",
    "Seja $ \\mbf A $ a matriz $ (M + 1) \\times 2 $ seguinte:\n",
    "$$\n",
    "\\mbf A =\n",
    "\\begin{bmatrix}\n",
    " \\vert & \\vert \\\\\n",
    " \\mbf x & \\mbf u \\\\\n",
    " \\vert & \\vert\n",
    "\\end{bmatrix}\\,.\n",
    "$$\n",
    "A imagem de $ \\mbf A $ coincide com o $ 2 $-plano $ P $ gerado por $ \\mbf x $ e $ \\mbf u $ em $ \\mathbb R^{m+1} $. Como acima, seja $ \\mbf {\\bar y} $ a projeção ortogonal de $ \\mbf y $ em $ P $, e seja $ \\mbf v = \\big( \\hat a, \\hat b \\big) $, de modo que $ \\mbf A \\mbf v = \\mbf {\\bar y} $. Então $ \\mbf v $ é o único vetor em $ \\mathbb R^2 $ tal que\n",
    "$$\n",
    "\\mbf y - \\mbf A \\mbf v = \\mbf y - \\mbf {\\bar y} = \\mbf y^\\perp\n",
    "$$\n",
    "é perpendicular a $ P $, ou seja, ortogonal tanto a $ \\mbf u $ quanto a $ \\mbf x $. Em termos de $ \\mbf A $, esta última condição é equivalente a:\n",
    "$$\n",
    "\\mbf A^t\\big(\\mbf y - \\mbf A \\mbf v\\big) = \\mbf 0\\,, \\quad \\text{ou seja}\\,, \\quad\n",
    "\\mbf A^t \\mbf A \\mbf v = \\mbf A^t \\mbf y\n",
    "$$\n",
    "e portanto\n",
    "$$\n",
    "\\boxed{\\mbf v = (\\mbf A^t \\mbf A)^{-1} \\mbf A^t \\mbf y} \\quad \\text{e} \\quad \\mbf A \\mbf v = \\boxed{\\mbf {\\bar y} = \\mbf A\\,(\\mbf A^t \\mbf A)^{-1} \\mbf A^t \\mbf y}\n",
    "$$\n",
    "\n",
    "⚠️ Note que $ A $ é uma matriz $ (M + 1) \\times (n + 1) $, portanto em geral sequer faz sentido falar da inversa de $ \\mbf A $. Em particular, *não* podemos expressar $ \\mbf v $ simplesmente como $ \\mbf A^{-1} \\mbf y $. Contudo, $ \\mbf A^t\\mbf A $ é uma matriz simétrica de dimensões $ (n + 1) \\times (n + 1) $. Ela é invertível desde que seja possível extrair um subconjunto de $ \\left\\{x_0,x_1,\\cdots,x_M\\right\\} $ com $ n + 1 $ valores $ x_i $ mutuamente distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0e0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
